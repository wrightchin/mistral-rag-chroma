[Document(page_content='NVIDIA GPU  architecture overview\nNVIDIA GPU prerequisites\nNVIDIA GPU enablement\nGPUs and bare metal\nGPUs and virtualization\nGPUs and vSphere\nGPUs and Red Hat KVM\nGPUs and CSPs\nGPUs and Red Hat Device Edge\nGPU sharing methods\nCUDA streams\nTime-slicing\nCUDA Multi-Process Service\nMulti-instance GPU\nVirtualization with vGPU\nNVIDIA GPU features for OpenShift Container Platform\nNVIDIA supports the use of graphics processing unit (GPU) resources on OpenShift', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 0}),
 Document(page_content='Container Platform. OpenShift Container Platform is a security-focused and\nhardened Kubernetes platform developed and supported by Red Hat for deploying\nand managing Kubernetes clusters at scale. OpenShift Container Platform includes\nenhancements to Kubernetes so that users can easily configure and use NVIDIA\nGPU resources to accelerate workloads.\nThe NVIDIA GPU Operator leverages the Operator framework within OpenShift', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 0}),
 Document(page_content='Container Platform to manage the full lifecycle of NVIDIA software components\nrequired to run GPU-accelerated workloads.\nThese components include the NVIDIA drivers (to enable CUDA), the Kubernetes\ndevice plugin for GPUs, the NVIDIA Container Toolkit, automatic node tagging using\nGPU feature discovery (GFD), DCGM-based monitoring, and others.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 0}),
 Document(page_content='https://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 1/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 0}),
 Document(page_content='\uf05aThe NVIDIA GPU Operator is only supported by NVIDIA. For more\ninformation about obtaining support from NVIDIA, see Obtaining\nSupport from NVIDIA.\nNVIDIA GPU prerequisites\nA working OpenShift cluster with at least one GPU worker node.\nAccess to the OpenShift cluster as a cluster-admin to perform the required\nsteps.\nOpenShift CLI (oc) is installed.\nThe node feature discovery (NFD) Operator is installed and a\nnodefeaturediscovery instance is created.\nNVIDIA GPU enablement', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 1}),
 Document(page_content='NVIDIA GPU enablement\nThe following diagram shows how the GPU architecture is enabled for OpenShift:\nFigure 1. NVIDIA GPU enablement\n\uf05aMIG is only supported with A30, A100, A100X, A800, AX800, H100,\nand H800.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 2/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 1}),
 Document(page_content='GPUs and bare metal\nYou can deploy OpenShift Container Platform on an NVIDIA-certified bare metal\nserver but with some limitations:\nControl plane nodes can be CPU nodes.\nWorker nodes must be GPU nodes, provided that AI/ML workloads are\nexecuted on these worker nodes.\nIn addition, the worker nodes can host one or more GPUs, but they must be of\nthe same type. For example, a node can have two NVIDIA A100 GPUs, but a\nnode with one A100 GPU and one T4 GPU is not supported. The NVIDIA', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 2}),
 Document(page_content='Device Plugin for Kubernetes does not support mixing different GPU models\non the same node.\nWhen using OpenShift, note that one or three or more servers are required.\nClusters with two servers are not supported. The single server deployment is\ncalled single node openShift (SNO) and using this configuration results in a\nnon-high availability OpenShift environment.\nYou can choose one of the following methods to access the containerized GPUs:\nGPU passthrough\nMulti-Instance GPU (MIG)', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 2}),
 Document(page_content='GPU passthrough\nMulti-Instance GPU (MIG)\nAdditional resources\nRed Hat OpenShift on Bare Metal Stack\nGPUs and virtualization\nMany developers and enterprises are moving to containerized applications and\nserverless infrastructures, but there is still a lot of interest in developing and\nmaintaining applications that run on virtual machines (VMs). Red Hat OpenShift\nVirtualization provides this capability, enabling enterprises to incorporate VMs into\ncontainerized workflows within clusters.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 2}),
 Document(page_content='containerized workflows within clusters.\nYou can choose one of the following methods to connect the worker nodes to the\nGPUs:10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 3/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 2}),
 Document(page_content='GPU passthrough to access and use GPU hardware within a virtual machine\n(VM).\nGPU (vGPU) time-slicing, when GPU compute capacity is not saturated by\nworkloads.\nAdditional resources\nNVIDIA GPU Operator with OpenShift Virtualization\nGPUs and vSphere\nYou can deploy OpenShift Container Platform on an NVIDIA-certified VMware\nvSphere server that can host different GPU types.\nAn NVIDIA GPU driver must be installed in the hypervisor in case vGPU instances', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 3}),
 Document(page_content='are used by the VMs. For VMware vSphere, this host driver is provided in the form of\na VIB file.\nThe maximum number of vGPUS that can be allocated to worker node VMs\ndepends on the version of vSphere:\nvSphere 7.0: maximum 4 vGPU per VM\nvSphere 8.0: maximum 8 vGPU per VM\n\uf05avSphere 8.0 introduced support for multiple full or fractional\nheterogenous profiles associated with a VM.\nYou can choose one of the following methods to attach the worker nodes to the\nGPUs:', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 3}),
 Document(page_content='GPUs:\nGPU passthrough for accessing and using GPU hardware within a virtual\nmachine (VM)\nGPU (vGPU) time-slicing, when not all of the GPU is needed\nSimilar to bare metal deployments, one or three or more servers are required.\nClusters with two servers are not supported.\nAdditional resources\nOpenShift Container Platform on VMware vSphere with NVIDIA vGPUs10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 3}),
 Document(page_content='https://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 4/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 3}),
 Document(page_content='GPUs and Red Hat KVM\nYou can use OpenShift Container Platform on an NVIDIA-certified kernel-based\nvirtual machine (KVM) server.\nSimilar to bare-metal deployments, one or three or more servers are required.\nClusters with two servers are not supported.\nHowever, unlike bare-metal deployments, you can use different types of GPUs in\nthe server. This is because you can assign these GPUs to different VMs that act as\nKubernetes nodes. The only limitation is that a Kubernetes node must have the', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 4}),
 Document(page_content='same set of GPU types at its own level.\nYou can choose one of the following methods to access the containerized GPUs:\nGPU passthrough for accessing and using GPU hardware within a virtual\nmachine (VM)\nGPU (vGPU) time-slicing when not all of the GPU is needed\nTo enable the vGPU capability, a special driver must be installed at the host level.\nThis driver is delivered as a RPM package. This host driver is not required at all for\nGPU passthrough allocation.\nAdditional resources', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 4}),
 Document(page_content='GPU passthrough allocation.\nAdditional resources\nHow To Deploy OpenShift Container Platform 4.13 on KVM\nGPUs and CSPs\nYou can deploy OpenShift Container Platform to one of the major cloud service\nproviders (CSPs): Amazon Web Services (AWS), Google Cloud Platform (GCP), or\nMicrosoft Azure.\nTwo modes of operation are available: a fully managed deployment and a self-\nmanaged deployment.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 4}),
 Document(page_content='https://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 5/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 4}),
 Document(page_content='In a fully managed deployment, everything is automated by Red Hat in\ncollaboration with CSP. You can request an OpenShift instance through the\nCSP web console, and the cluster is automatically created and fully managed\nby Red Hat. You do not have to worry about node failures or errors in the\nenvironment. Red Hat is fully responsible for maintaining the uptime of the\ncluster. The fully managed services are available on AWS and Azure. For AWS,', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 5}),
 Document(page_content='the OpenShift service is called ROSA (Red Hat OpenShift Service on AWS).\nFor Azure, the service is called Azure Red Hat OpenShift.\nIn a self-managed deployment, you are responsible for instantiating and\nmaintaining the OpenShift cluster. Red Hat provides the OpenShift-install\nutility to support the deployment of the OpenShift cluster in this case. The\nself-managed services are available globally to all CSPs.\nIt is important that this compute instance is a GPU-accelerated compute instance', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 5}),
 Document(page_content='and that the GPU type matches the list of supported GPUs from NVIDIA AI\nEnterprise. For example, T4, V100, and A100 are part of this list.\nYou can choose one of the following methods to access the containerized GPUs:\nGPU passthrough to access and use GPU hardware within a virtual machine\n(VM).\nGPU (vGPU) time slicing when the entire GPU is not required.\nAdditional resources\nRed Hat Openshift in the Cloud\nGPUs and Red Hat Device Edge', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 5}),
 Document(page_content='GPUs and Red Hat Device Edge\nRed Hat Device Edge provides access to MicroShift. MicroShift provides the\nsimplicity of a single-node deployment with the functionality and services you need\nfor resource-constrained (edge) computing. Red Hat Device Edge meets the needs\nof bare-metal, virtual, containerized, or Kubernetes workloads deployed in resource-\nconstrained environments.\nYou can enable NVIDIA GPUs on containers in a Red Hat Device Edge environment.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 5}),
 Document(page_content='You use GPU passthrough to access the containerized GPUs.\nAdditional resources10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 6/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 5}),
 Document(page_content='How to accelerate workloads with NVIDIA GPUs on Red Hat Device Edge\nGPU sharing methods\nRed\xa0Hat and NVIDIA have developed GPU concurrency and sharing mechanisms to\nsimplify GPU-accelerated computing on an enterprise-level OpenShift Container\nPlatform cluster.\nApplications typically have different compute requirements that can leave GPUs\nunderutilized. Providing the right amount of compute resources for each workload is\ncritical to reduce deployment cost and maximize GPU utilization.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 6}),
 Document(page_content='Concurrency mechanisms for improving GPU utilization exist that range from\nprogramming model APIs to system software and hardware partitioning, including\nvirtualization. The following list shows the GPU concurrency mechanisms:\nCompute Unified Device Architecture (CUDA) streams\nTime-slicing\nCUDA Multi-Process Service (MPS)\nMulti-instance GPU (MIG)\nVirtualization with vGPU\nConsider the following GPU sharing suggestions when using the GPU concurrency', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 6}),
 Document(page_content='mechanisms for different OpenShift Container Platform scenarios:\nBare metal\nvGPU is not available. Consider using MIG-enabled cards.\nVMs\nvGPU is the best choice.\nOlder NVIDIA cards with no MIG on bare metal\nConsider using time-slicing.\nVMs with multiple GPUs and you want passthrough and vGPU\nConsider using separate VMs.\nBare metal with OpenShift Virtualization and multiple GPUs', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 6}),
 Document(page_content='Consider using pass-through for hosted VMs and time-slicing for containers.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 7/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 6}),
 Document(page_content='Additional resources\nImproving GPU Utilization\nCUDA streams\nCompute Unified Device Architecture (CUDA) is a parallel computing platform and\nprogramming model developed by NVIDIA for general computing on GPUs.\nA stream is a sequence of operations that executes in issue-order on the GPU.\nCUDA commands are typically executed sequentially in a default stream and a task\ndoes not start until a preceding task has completed.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 7}),
 Document(page_content='Asynchronous processing of operations across different streams allows for parallel\nexecution of tasks. A task issued in one stream runs before, during, or after another\ntask is issued into another stream. This allows the GPU to run multiple tasks\nsimultaneously in no prescribed order, leading to improved performance.\nAdditional resources\nAsynchronous Concurrent Execution\nTime-slicing\nGPU time-slicing interleaves workloads scheduled on overloaded GPUs when you', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 7}),
 Document(page_content='are running multiple CUDA applications.\nYou can enable time-slicing of GPUs on Kubernetes by defining a set of replicas for\na GPU, each of which can be independently distributed to a pod to run workloads\non. Unlike multi-instance GPU (MIG), there is no memory or fault isolation between\nreplicas, but for some workloads this is better than not sharing at all. Internally, GPU\ntime-slicing is used to multiplex workloads from replicas of the same underlying\nGPU.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 7}),
 Document(page_content='GPU.\nYou can apply a cluster-wide default configuration for time-slicing. You can also\napply node-specific configurations. For example, you can apply a time-slicing\nconfiguration only to nodes with Tesla T4 GPUs and not modify nodes with other\nGPU models.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 8/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 7}),
 Document(page_content='You can combine these two approaches by applying a cluster-wide default\nconfiguration and then labeling nodes to give those nodes a node-specific\nconfiguration.\nCUDA Multi-Process Service\nCUDA Multi-Process Service (MPS) allows a single GPU to use multiple CUDA\nprocesses. The processes run in parallel on the GPU, eliminating saturation of the\nGPU compute resources. MPS also enables concurrent execution, or overlapping, of\nkernel operations and memory copying from different processes to enhance', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 8}),
 Document(page_content='utilization.\nAdditional resources\nCUDA MPS\nMulti-instance GPU\nUsing Multi-instance GPU (MIG), you can split GPU compute units and memory into\nmultiple MIG instances. Each of these instances represents a standalone GPU\ndevice from a system perspective and can be connected to any application,\ncontainer, or virtual machine running on the node. The software that uses the GPU\ntreats each of these MIG instances as an individual GPU.', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 8}),
 Document(page_content='MIG is useful when you have an application that does not require the full power of an\nentire GPU. The MIG feature of the new NVIDIA Ampere architecture enables you to\nsplit your hardware resources into multiple GPU instances, each of which is available\nto the operating system as an independent CUDA-enabled GPU.\nNVIDIA GPU Operator version 1.7.0 and higher provides MIG support for the A100\nand A30 Ampere cards. These GPU instances are designed to support up to seven', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 8}),
 Document(page_content='multiple independent CUDA applications so that they operate completely isolated\nwith dedicated hardware resources.\nAdditional resources\nNVIDIA Multi-Instance GPU User Guide10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 9/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 8}),
 Document(page_content='Virtualization with vGPU\nVirtual machines (VMs) can directly access a single physical GPU using NVIDIA\nvGPU. You can create virtual GPUs that can be shared by VMs across the enterprise\nand accessed by other devices.\nThis capability combines the power of GPU performance with the management and\nsecurity benefits provided by vGPU. Additional benefits provided by vGPU includes\nproactive management and monitoring for your VM environment, workload', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 9}),
 Document(page_content='balancing for mixed VDI and compute workloads, and resource sharing across\nmultiple VMs.\nAdditional resources\nVirtual GPUs\nNVIDIA GPU features for OpenShift Container Platform\nNVIDIA Container Toolkit\nNVIDIA Container Toolkit enables you to create and run GPU-accelerated\ncontainers. The toolkit includes a container runtime library and utilities to\nautomatically configure containers to use NVIDIA GPUs.\nNVIDIA AI Enterprise', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 9}),
 Document(page_content='NVIDIA AI Enterprise\nNVIDIA AI Enterprise is an end-to-end, cloud-native suite of AI and data analytics\nsoftware optimized, certified, and supported with NVIDIA-Certified systems.\nNVIDIA AI Enterprise includes support for Red Hat OpenShift Container Platform.\nThe following installation methods are supported:\nOpenShift Container Platform on bare metal or VMware vSphere with GPU\nPassthrough.\nOpenShift Container Platform on VMware vSphere with NVIDIA vGPU.\nGPU Feature Discovery', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 9}),
 Document(page_content='GPU Feature Discovery\nNVIDIA GPU Feature Discovery for Kubernetes is a software component that\nenables you to automatically generate labels for the GPUs available on a node. GPU\nFeature Discovery uses node feature discovery (NFD) to perform this labeling.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 10/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 9}),
 Document(page_content='The Node Feature Discovery Operator (NFD) manages the discovery of hardware\nfeatures and configurations in an OpenShift Container Platform cluster by labeling\nnodes with hardware-specific information. NFD labels the host with node-specific\nattributes, such as PCI cards, kernel, OS version, and so on.\nYou can find the NFD Operator in the Operator Hub by searching for “Node Feature\nDiscovery”.\nNVIDIA GPU Operator with OpenShift Virtualization', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 10}),
 Document(page_content='NVIDIA GPU Operator with OpenShift Virtualization\nUp until this point, the GPU Operator only provisioned worker nodes to run GPU-\naccelerated containers. Now, the GPU Operator can also be used to provision\nworker nodes for running GPU-accelerated virtual machines (VMs).\nYou can configure the GPU Operator to deploy different software components to\nworker nodes depending on which GPU workload is configured to run on those\nnodes.\nGPU Monitoring dashboard', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 10}),
 Document(page_content='nodes.\nGPU Monitoring dashboard\nYou can install a monitoring dashboard to display GPU usage information on the\ncluster Observe page in the OpenShift Container Platform web console. GPU\nutilization information includes the number of available GPUs, power consumption\n(in watts), temperature (in degrees Celsius), utilization (in percent), and other\nmetrics for each GPU.\nAdditional resources\nNVIDIA-Certified Systems\nNVIDIA AI Enterprise\nNVIDIA Container Toolkit\nEnabling the GPU Monitoring Dashboard', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 10}),
 Document(page_content='Enabling the GPU Monitoring Dashboard\nMIG Support in OpenShift Container Platform\nTime-slicing NVIDIA GPUs in OpenShift\nDeploy GPU Operators in a disconnected or airgapped environment\nNode Feature Discovery Operator\nCopyright © 2024 Red Hat, Inc.10/06/2024, 09:45 NVIDIA GPU architecture overview | Architecture | OpenShift Container Platform 4.14\nhttps://docs.openshift.com/container-platform/4.14/architecture/nvidia-gpu-architecture-overview.html 11/11', metadata={'source': 'mistral/nvidia_ocp_414.pdf', 'page': 10})]